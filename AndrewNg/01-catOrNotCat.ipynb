{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will determine if a given image is of a cat or not. Images are obtained form [cifar-10](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_ref/img.png\" alt=\"Image description\" style=\"width:20%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have $m$ such images,\n",
    "\\begin{equation*}\n",
    "\\text{Training Set} :\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\cdots ,(x^{(m)},y^{(m)})\\}\n",
    "\\end{equation*}\n",
    "\n",
    "We separate the image and label in matrix form because it is computationally more efficient. So\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  X &= \\begin{bmatrix}\n",
    "            \\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "            x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "            \\vdots & \\vdots & \\cdots & \\vdots\n",
    "        \\end{bmatrix}_{n_x \\times m} \\nonumber \\\\\n",
    "  Y &= \\begin{bmatrix}\n",
    "            y^{(1)} & y^{(2)} & \\cdots & y^{(m)}\n",
    "        \\end{bmatrix}_{1 \\times m} \\nonumber \\\\\n",
    "  X &\\in \\mathbb{R}^{n_x \\times m}, \\quad Y \\in \\mathbb{R}^{1 \\times m}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a9e26ac570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics import ConfusionMatrix\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " m: 10000\n",
      "nx: 3072\n",
      "shape of X: torch.Size([3072, 10000])\n",
      "shape of Y: torch.Size([1, 10000])\n",
      "Random No: 6044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(31.5), np.float64(31.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "dataDict = unpickle(\"../raw/cifar-10-batches-py/data_batch_1\")\n",
    "\n",
    "# print(dataDict.keys())\n",
    "X=torch.tensor(dataDict[b'data']).T\n",
    "X=X.to(dtype=torch.float32)/255  # better to convert the range to 0 to 1 instead of 0 to 255 \n",
    "X = 2*X-1\n",
    "labels=torch.tensor(dataDict[b'labels']).view(1,-1)\n",
    "Y = (labels==3).to(dtype=torch.float) ## 0 if image is not cat 1 if it is cat\n",
    "# labels=\n",
    "m=len(Y[0])  # We will use all these images so this is m training test case\n",
    "nx = len(X)\n",
    "\n",
    "print(f\"labels:\",list(set(dataDict[b'labels'])))\n",
    "print(f\" m: {m}\")\n",
    "print(f\"nx: {nx}\")\n",
    "print(f\"shape of X: {X.shape}\")\n",
    "print(f\"shape of Y: {Y.shape}\")\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "fig.set_size_inches(2,2)\n",
    "randomImageNum = torch.randint(0,m,(1,)).item()\n",
    "print(f\"Random No: {randomImageNum}\")\n",
    "ax.imshow((X[:,randomImageNum].view(-1,1024).T.view(32,32,3)+1)/2)\n",
    "ax.set_title(f\"{Y[0,randomImageNum]}\")\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for a given image we want to calculate probability that it is of a cat. Mathematically we can write\n",
    "\\begin{equation}\n",
    "\\text{given}\\quad x\\in\\mathbb{R}^{{n_x}\\times 1}, \\text{want}\\quad \\hat{y}=P(y=1|x), \\quad 0 \\leq \\hat{y} \\leq 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "z=w^T\\cdot x+b \\quad|w\\in\\mathbb{R}^{n_x},  b \\in \\mathbb{R}\n",
    "\\end{equation}\n",
    "In fancy term these $w$ is called \\textit{weights} and $b$ is called \\textit{bias}.\n",
    "Also we want z to be in the [0,1] range so we add activation function \n",
    "\\begin{equation} \n",
    "a=\\hat{y}=\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "Loss/Error Functoin\n",
    "\n",
    "\\begin{equation} \n",
    "L(\\hat{y},y)= - (y\\log{ (\\hat{y})  }+(1-y)\\log{(1-\\hat{y})})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Cost Function\n",
    "\n",
    "So our task is to find $w,b$ which minimizes cost $J(w,b)$.\n",
    "\\begin{equation}\n",
    "J(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}{L(\\hat{y}^{(i)},y^{(i)})}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,A):\n",
    "    # return (y*y_hat)**2\n",
    "    return -(y*torch.log(A)+(1-y)*torch.log(1-A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z):\n",
    "    return 1/(1+torch.exp(-z))\n",
    "    # return torch.tanh(z)\n",
    "    # return torch.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y,A):\n",
    "    A_= (A>0.9).to(dtype=torch.float)\n",
    "    A_= torch.abs(y-A_)<1e-4\n",
    "    return A_.sum()/A_.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " m: 10000\n",
      "nx: 3072\n",
      "shape of X: torch.Size([3072, 10000])\n",
      "shape of Y: torch.Size([1, 10000])\n",
      "shape of Z: torch.Size([1, 10000])\n",
      "shape of A: torch.Size([1, 10000])\n",
      "shape of L: torch.Size([1, 10000])\n",
      "shape of J: torch.Size([])\n",
      "value of J: 1.7625203132629395\n",
      "value of z: tensor([-2.0119,  1.1864,  1.5722, -4.7944, -2.1123, -3.6311, -0.0662,  0.2167,\n",
      "         3.6946, -5.8888, -5.5305,  2.5801,  1.4581, -5.2086, -3.1885,  2.4981,\n",
      "         2.3088, -3.7344,  0.8785, -0.1498], grad_fn=<SliceBackward0>)\n",
      "value of A: tensor([0.1180, 0.7661, 0.8281, 0.0082, 0.1079, 0.0258, 0.4835, 0.5540, 0.9757,\n",
      "        0.0028, 0.0039, 0.9296, 0.8112, 0.0054, 0.0396, 0.9240, 0.9096, 0.0233,\n",
      "        0.7065, 0.4626], grad_fn=<SliceBackward0>)\n",
      "value of Y: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jangi\\AppData\\Local\\Temp\\ipykernel_18916\\274238781.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w=torch.tensor(torch.rand(nx,1)*0.01,requires_grad=True)\n",
      "C:\\Users\\jangi\\AppData\\Local\\Temp\\ipykernel_18916\\274238781.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(torch.rand(1),requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\" m: {m}\")\n",
    "print(f\"nx: {nx}\")\n",
    "print(f\"shape of X: {X.shape}\")\n",
    "print(f\"shape of Y: {Y.shape}\")\n",
    "\n",
    "## initialize weights and bias matrx\n",
    "\n",
    "# w = torch.rand(nx,1,requires_grad=True)*0.01 # DO NOT EVER DO THIS\n",
    "w=torch.tensor(torch.rand(nx,1)*0.01,requires_grad=True)\n",
    "b = torch.tensor(torch.rand(1),requires_grad=True)\n",
    "z = w.T @ X + b\n",
    "A = activation(z)\n",
    "L = loss(Y,A)\n",
    "J = torch.sum(L)/m\n",
    "print(f\"shape of Z: {z.shape}\")\n",
    "print(f\"shape of A: {A.shape}\")\n",
    "print(f\"shape of L: {L.shape}\")\n",
    "print(f\"shape of J: {J.shape}\")\n",
    "print(f\"value of J: {J}\")\n",
    "\n",
    "print(f\"value of z: {z[0,0:20]}\")\n",
    "print(f\"value of A: {A[0,0:20]}\")\n",
    "print(f\"value of Y: {Y[0,0:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                  | **Actual Positive** | **Actual Negative** |\n",
    "|------------------|---------------------|---------------------|\n",
    "| **Predicted Positive** | True Positive (TP)     | False Positive (FP)    |\n",
    "| **Predicted Negative** | False Negative (FN)    | True Negative (TN)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0   Cost: 0.34   Accucary:0.88\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8660, FP: 867\n",
      "FN: 324, TP: 149\n",
      "\n",
      "Iteration: 500 Cost: 0.33   Accucary:0.88\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8687, FP: 875\n",
      "FN: 297, TP: 141\n",
      "\n",
      "Iteration: 1000 Cost: 0.33   Accucary:0.88\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8712, FP: 882\n",
      "FN: 272, TP: 134\n",
      "\n",
      "Iteration: 1500 Cost: 0.33   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8734, FP: 887\n",
      "FN: 250, TP: 129\n",
      "\n",
      "Iteration: 2000 Cost: 0.32   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8760, FP: 894\n",
      "FN: 224, TP: 122\n",
      "\n",
      "Iteration: 2500 Cost: 0.32   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8778, FP: 901\n",
      "FN: 206, TP: 115\n",
      "\n",
      "Iteration: 3000 Cost: 0.32   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8795, FP: 905\n",
      "FN: 189, TP: 111\n",
      "\n",
      "Iteration: 3500 Cost: 0.31   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8815, FP: 910\n",
      "FN: 169, TP: 106\n",
      "\n",
      "Iteration: 4000 Cost: 0.31   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8827, FP: 916\n",
      "FN: 157, TP: 100\n",
      "\n",
      "Iteration: 4500 Cost: 0.31   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8832, FP: 921\n",
      "FN: 152, TP:  95\n",
      "\n",
      "Iteration: 5000 Cost: 0.31   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8841, FP: 924\n",
      "FN: 143, TP:  92\n",
      "\n",
      "Iteration: 5500 Cost: 0.31   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8845, FP: 927\n",
      "FN: 139, TP:  89\n",
      "\n",
      "Iteration: 6000 Cost: 0.30   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8853, FP: 928\n",
      "FN: 131, TP:  88\n",
      "\n",
      "Iteration: 6500 Cost: 0.30   Accucary:0.89\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8860, FP: 930\n",
      "FN: 124, TP:  86\n",
      "\n",
      "Iteration: 7000 Cost: 0.30   Accucary:0.90\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8868, FP: 933\n",
      "FN: 116, TP:  83\n",
      "\n",
      "Iteration: 7500 Cost: 0.30   Accucary:0.90\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8877, FP: 935\n",
      "FN: 107, TP:  81\n",
      "\n",
      "Iteration: 8000 Cost: 0.30   Accucary:0.90\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8885, FP: 935\n",
      "FN:  99, TP:  81\n",
      "\n",
      "Iteration: 8500 Cost: 0.30   Accucary:0.90\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8891, FP: 937\n",
      "FN:  93, TP:  79\n",
      "\n",
      "Iteration: 9000 Cost: 0.30   Accucary:0.90\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8895, FP: 941\n",
      "FN:  89, TP:  75\n",
      "\n",
      "Iteration: 9500 Cost: 0.30   Accucary:0.90\n",
      "minA: 0.00, maxA 1.00\n",
      "TN:8900, FP: 943\n",
      "FN:  84, TP:  73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha=0.001\n",
    "confmat = ConfusionMatrix(task=\"binary\",num_classes=2)\n",
    "for i in range(10000):\n",
    "    J.backward()\n",
    "    with torch.no_grad():  # Disable gradient tracking during update\n",
    "        w -= alpha * w.grad\n",
    "        b -= alpha * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    z=w.T@X+b\n",
    "    A = activation(z)\n",
    "    L = loss(Y,A)\n",
    "    J = torch.sum(L)/m\n",
    "    if i%500 == 0:\n",
    "        # print(A[0,0:20])\n",
    "        # print(Y[0,0:20])\n",
    "        # Compute confusion matrix\n",
    "        cm = confmat(Y, (A>0.5).to(dtype=torch.float32))\n",
    "        accuracy = (cm[0,0]+cm[1,1])/torch.sum(cm)\n",
    "        print(f\"Iteration: {i:<3d} Cost: {J:<6.2f} Accucary:{accuracy:.2f}\")\n",
    "        print(f\"minA: {torch.min(A).item():0.2f}, maxA {torch.max(A).item():0.2f}\")\n",
    "        \n",
    "        print(f\"TN:{cm[0,0]:>4}, FP:{cm[0,1]:>4}\")\n",
    "        print(f\"FN:{cm[1,0]:>4}, TP:{cm[1,1]:>4}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
