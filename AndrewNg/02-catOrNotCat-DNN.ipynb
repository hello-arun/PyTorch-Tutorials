{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Plan to use deep neural network to achieve better accuray. I will take 1 input layer 3 hidden layers and one final output layer.\n",
    "\n",
    "nx -> 10 -> 10 -> 10 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics import ConfusionMatrix\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "       Counts: tensor([1005,  974, 1032, 1016,  999,  937, 1030, 1001, 1025,  981])\n",
      "Total Cats: 1016\n",
      "labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " m: 18128\n",
      "nx: 3072\n",
      "shape of X: torch.Size([3072, 18128])\n",
      "shape of Y: torch.Size([1, 18128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "dataDict = unpickle(\"../raw/cifar-10-batches-py/data_batch_1\")\n",
    "\n",
    "# print(dataDict.keys())\n",
    "X=torch.tensor(dataDict[b'data']).T\n",
    "labels=torch.tensor(dataDict[b'labels']).view(1,-1)\n",
    "unique_values, counts = torch.unique(labels, return_counts=True)\n",
    "# Print results\n",
    "print(\"Unique values:\", unique_values)\n",
    "print(\"       Counts:\", counts)\n",
    "\n",
    "X=X.to(dtype=torch.float32)/255  # better to convert the range to 0 to 1 instead of 0 to 255 \n",
    "X = 2*X-1\n",
    "Y = (labels==3).to(dtype=torch.float) ## 0 if image is not cat 1 if it is cat\n",
    "\n",
    "m=len(Y[0])\n",
    "nx = len(X)\n",
    "\n",
    "catIndicies=torch.nonzero((labels==3))\n",
    "print(f\"Total Cats: {catIndicies.shape[0]}\")\n",
    "catsData=X[:,catIndicies[:,1]]\n",
    "catsLabels=Y[:,catIndicies[:,1]]\n",
    "\n",
    "catsDataR=catsData.repeat(1,8)\n",
    "catsLabels=catsLabels.repeat(1,8)\n",
    "\n",
    "X = torch.cat((X,catsDataR),dim=1).to(device=device)\n",
    "Y = torch.cat((Y,catsLabels),dim=1).to(device=device)\n",
    "m=len(Y[0])\n",
    "nx = len(X)\n",
    "\n",
    "# print(catsData)\n",
    "print(f\"labels:\",list(set(dataDict[b'labels'])))\n",
    "print(f\" m: {m}\")\n",
    "print(f\"nx: {nx}\")\n",
    "print(f\"shape of X: {X.shape}\")\n",
    "print(f\"shape of Y: {Y.shape}\")\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "# fig.set_size_inches(2,2)\n",
    "# randomImageNum = torch.randint(0,m,(1,)).item()\n",
    "# print(f\"Random No: {randomImageNum}\")\n",
    "# ax.imshow((X[:,randomImageNum].view(-1,1024).T.view(32,32,3)+1)/2)\n",
    "# ax.set_title(f\"{Y[0,randomImageNum]}\")\n",
    "# ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,A):\n",
    "    # return (y*y_hat)**2\n",
    "    return -(y*torch.log(A)+(1-y)*torch.log(1-A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " m: 18128\n",
      "nx: 3072\n",
      "shape of X: torch.Size([3072, 18128])\n",
      "shape of Y: torch.Size([1, 18128])\n",
      "value of J: 0.7379055619239807\n"
     ]
    }
   ],
   "source": [
    "neurons = [nx,128,64,32,1]\n",
    "\n",
    "print(f\" m: {m}\")\n",
    "print(f\"nx: {nx}\")\n",
    "print(f\"shape of X: {X.shape}\")\n",
    "print(f\"shape of Y: {Y.shape}\")\n",
    "\n",
    "## initialize weights and bias matrx\n",
    "w0 = torch.rand(size=(neurons[0], neurons[1]),device=device) * 0.01\n",
    "w1 = torch.rand(size=(neurons[1], neurons[2]),device=device) * 0.01\n",
    "w2 = torch.rand(size=(neurons[2], neurons[3]),device=device) * 0.01\n",
    "w3 = torch.rand(size=(neurons[3], neurons[4]),device=device) * 0.01\n",
    "b0 = torch.rand(size=(neurons[1],1),device=device)*1.0\n",
    "b1 = torch.rand(size=(neurons[2],1),device=device)*1.0\n",
    "b2 = torch.rand(size=(neurons[3],1),device=device)*1.0\n",
    "b3 = torch.rand(size=(neurons[4],1),device=device)*1.0\n",
    "\n",
    "# Always put this at end \n",
    "w0.requires_grad_(True)\n",
    "w1.requires_grad_(True)\n",
    "w2.requires_grad_(True)\n",
    "w3.requires_grad_(True)\n",
    "b0.requires_grad_(True)\n",
    "b1.requires_grad_(True)\n",
    "b2.requires_grad_(True)\n",
    "b3.requires_grad_(True)\n",
    "\n",
    "z1 = w0.T @ X +b0\n",
    "A1 = torch.relu(z1)\n",
    "\n",
    "z2 = w1.T@A1 + b1\n",
    "A2 = torch.relu(z2)\n",
    "\n",
    "z3 = w2.T@A2 + b2\n",
    "A3 = torch.relu(z3)\n",
    "\n",
    "z4 = w3.T@A3 + b3\n",
    "A4 = torch.sigmoid(z4)\n",
    "\n",
    "\n",
    "L = loss(Y,A4)\n",
    "J = torch.sum(L)/m\n",
    "print(f\"value of J: {J}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                  | **Actual Positive** | **Actual Negative** |\n",
    "|------------------|---------------------|---------------------|\n",
    "| **Predicted Positive** | True Positive (TP)     | False Positive (FP)    |\n",
    "| **Predicted Negative** | False Negative (FN)    | True Negative (TN)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0   Cost: 0.73   Accucary:0.50\n",
      "minA: 0.63, maxA 0.70\n",
      "TN:   0, FP:   0\n",
      "FN:8984, TP:9144\n",
      "\n",
      "Iteration: 200 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.25, maxA 0.53\n",
      "TN:2480, FP:1953\n",
      "FN:6504, TP:7191\n",
      "\n",
      "Iteration: 400 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.27, maxA 0.53\n",
      "TN:2537, FP:1989\n",
      "FN:6447, TP:7155\n",
      "\n",
      "Iteration: 600 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.29, maxA 0.53\n",
      "TN:2601, FP:2052\n",
      "FN:6383, TP:7092\n",
      "\n",
      "Iteration: 800 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.53\n",
      "TN:2622, FP:2052\n",
      "FN:6362, TP:7092\n",
      "\n",
      "Iteration: 1000 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2638, FP:2106\n",
      "FN:6346, TP:7038\n",
      "\n",
      "Iteration: 1200 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2648, FP:2106\n",
      "FN:6336, TP:7038\n",
      "\n",
      "Iteration: 1400 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2654, FP:2106\n",
      "FN:6330, TP:7038\n",
      "\n",
      "Iteration: 1600 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2657, FP:2124\n",
      "FN:6327, TP:7020\n",
      "\n",
      "Iteration: 1800 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2661, FP:2124\n",
      "FN:6323, TP:7020\n",
      "\n",
      "Iteration: 2000 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2667, FP:2124\n",
      "FN:6317, TP:7020\n",
      "\n",
      "Iteration: 2200 Cost: 0.69   Accucary:0.53\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2670, FP:2124\n",
      "FN:6314, TP:7020\n",
      "\n",
      "Iteration: 2400 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2673, FP:2115\n",
      "FN:6311, TP:7029\n",
      "\n",
      "Iteration: 2600 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2676, FP:2115\n",
      "FN:6308, TP:7029\n",
      "\n",
      "Iteration: 2800 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2677, FP:2115\n",
      "FN:6307, TP:7029\n",
      "\n",
      "Iteration: 3000 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.30, maxA 0.52\n",
      "TN:2682, FP:2115\n",
      "FN:6302, TP:7029\n",
      "\n",
      "Iteration: 3200 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.52\n",
      "TN:2685, FP:2106\n",
      "FN:6299, TP:7038\n",
      "\n",
      "Iteration: 3400 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.52\n",
      "TN:2686, FP:2106\n",
      "FN:6298, TP:7038\n",
      "\n",
      "Iteration: 3600 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.52\n",
      "TN:2690, FP:2115\n",
      "FN:6294, TP:7029\n",
      "\n",
      "Iteration: 3800 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.52\n",
      "TN:2692, FP:2133\n",
      "FN:6292, TP:7011\n",
      "\n",
      "Iteration: 4000 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.52\n",
      "TN:2694, FP:2133\n",
      "FN:6290, TP:7011\n",
      "\n",
      "Iteration: 4200 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.52\n",
      "TN:2695, FP:2133\n",
      "FN:6289, TP:7011\n",
      "\n",
      "Iteration: 4400 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.52\n",
      "TN:2698, FP:2124\n",
      "FN:6286, TP:7020\n",
      "\n",
      "Iteration: 4600 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.53\n",
      "TN:2701, FP:2115\n",
      "FN:6283, TP:7029\n",
      "\n",
      "Iteration: 4800 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.53\n",
      "TN:2703, FP:2115\n",
      "FN:6281, TP:7029\n",
      "\n",
      "Iteration: 5000 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.53\n",
      "TN:2707, FP:2115\n",
      "FN:6277, TP:7029\n",
      "\n",
      "Iteration: 5200 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.29, maxA 0.53\n",
      "TN:2711, FP:2106\n",
      "FN:6273, TP:7038\n",
      "\n",
      "Iteration: 5400 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2718, FP:2097\n",
      "FN:6266, TP:7047\n",
      "\n",
      "Iteration: 5600 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2724, FP:2097\n",
      "FN:6260, TP:7047\n",
      "\n",
      "Iteration: 5800 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2729, FP:2097\n",
      "FN:6255, TP:7047\n",
      "\n",
      "Iteration: 6000 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2731, FP:2097\n",
      "FN:6253, TP:7047\n",
      "\n",
      "Iteration: 6200 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2740, FP:2097\n",
      "FN:6244, TP:7047\n",
      "\n",
      "Iteration: 6400 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2747, FP:2097\n",
      "FN:6237, TP:7047\n",
      "\n",
      "Iteration: 6600 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2755, FP:2097\n",
      "FN:6229, TP:7047\n",
      "\n",
      "Iteration: 6800 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2762, FP:2097\n",
      "FN:6222, TP:7047\n",
      "\n",
      "Iteration: 7000 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2767, FP:2097\n",
      "FN:6217, TP:7047\n",
      "\n",
      "Iteration: 7200 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2772, FP:2097\n",
      "FN:6212, TP:7047\n",
      "\n",
      "Iteration: 7400 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.28, maxA 0.53\n",
      "TN:2774, FP:2097\n",
      "FN:6210, TP:7047\n",
      "\n",
      "Iteration: 7600 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.27, maxA 0.53\n",
      "TN:2778, FP:2106\n",
      "FN:6206, TP:7038\n",
      "\n",
      "Iteration: 7800 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.27, maxA 0.53\n",
      "TN:2785, FP:2106\n",
      "FN:6199, TP:7038\n",
      "\n",
      "Iteration: 8000 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.27, maxA 0.53\n",
      "TN:2788, FP:2115\n",
      "FN:6196, TP:7029\n",
      "\n",
      "Iteration: 8200 Cost: 0.69   Accucary:0.54\n",
      "minA: 0.27, maxA 0.53\n",
      "TN:2794, FP:2115\n",
      "FN:6190, TP:7029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha=0.01\n",
    "confmat = ConfusionMatrix(task=\"binary\",num_classes=2)\n",
    "for i in range(10000):\n",
    "    J.backward()\n",
    "    with torch.no_grad():  # Disable gradient tracking during update\n",
    "        w0 -= alpha * w0.grad\n",
    "        b0 -= alpha * b0.grad\n",
    "        \n",
    "        w1 -= alpha * w1.grad\n",
    "        b1 -= alpha * b1.grad\n",
    "\n",
    "        w2 -= alpha * w2.grad\n",
    "        b2 -= alpha * b2.grad\n",
    "\n",
    "        w3 -= alpha * w3.grad\n",
    "        b3 -= alpha * b3.grad\n",
    "        \n",
    "\n",
    "        w0.grad.zero_()\n",
    "        b0.grad.zero_()\n",
    "        w1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "        w3.grad.zero_()\n",
    "        b3.grad.zero_()\n",
    "\n",
    "    z1 = w0.T @ X +b0\n",
    "    A1 = torch.relu(z1)\n",
    "\n",
    "    z2 = w1.T@A1 + b1\n",
    "    A2 = torch.relu(z2)\n",
    "\n",
    "    z3 = w2.T@A2 + b2\n",
    "    A3 = torch.relu(z3)\n",
    "\n",
    "    z4 = w3.T@A3 + b3\n",
    "    A4 = torch.sigmoid(z4)\n",
    "\n",
    "    L = loss(Y,A4)\n",
    "    J = torch.sum(L)/m\n",
    "    if i%200 == 0:\n",
    "        torch.save({\n",
    "            'w0': w0, \n",
    "            'w1': w1,\n",
    "            'w2': w2,\n",
    "            'w3': w3,\n",
    "            'b0': b0, \n",
    "            'b1': b1,\n",
    "            'b2': b2,\n",
    "            'b3': b3,\n",
    "        }, f'./wbs/02-WsAndBs-{i}.pt')\n",
    "\n",
    "        Y_cpu=Y.cpu()\n",
    "        A4_cpu=A4.cpu()\n",
    "\n",
    "        cm = confmat(Y_cpu, (A4_cpu>0.5).float())\n",
    "        accuracy = (cm[0,0]+cm[1,1])/torch.sum(cm)\n",
    "        print(f\"Iteration: {i:<3d} Cost: {J:<6.2f} Accucary:{accuracy:.2f}\")\n",
    "        print(f\"minA: {torch.min(A4).item():0.2f}, maxA {torch.max(A4).item():0.2f}\")\n",
    "        \n",
    "        print(f\"TN:{cm[0,0]:>4}, FP:{cm[0,1]:>4}\")\n",
    "        print(f\"FN:{cm[1,0]:>4}, TP:{cm[1,1]:>4}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
